{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "       <th style=\"text-align:center;\"><h3>IS217 Practice 1 - Pandas</h3></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Outcomes\n",
    "\n",
    "At the end of this lesson, you should be able to:\n",
    "<ul>\n",
    "<li>Import Pandas Library</li>\n",
    "<li>Create and manipulate Pandas Series (with custom indices)</li>\n",
    "<li>Generate descriptive statistics for Pandas Series</li>\n",
    "<li>Create Pandas DataFrames (with custom indices)</li>\n",
    "<li>Access selectively some rows and columns of a DataFrame</li>\n",
    "<li>Generate descriptive statistics for Pandas DataFrames</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Pandas Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 0. Setup and Imports\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)# Set random seed as required (888)\n",
    "np.random.seed(888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set random seed as required (888)\n",
    "np.random.seed(888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. Data Loading and Cleaning\n",
    "# ==============================================================================\n",
    "print(\"Loading and cleaning data...\")\n",
    "try:\n",
    "    df = pd.read_csv('HDBResaleFlatPrices.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: HDBResaleFlatPrices.csv not found. Please ensure the file is in the directory.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
       "       'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. Feature Engineering & Transformation\n",
    "# ==============================================================================\n",
    "print(\"Starting Feature Engineering...\")\n",
    "\n",
    "# --- 2.1 Engineer Time-Based Features ---\n",
    "df['sale_year'] = pd.to_datetime(df['month'], format='%Y-%m').dt.year\n",
    "df['remaining_lease'] = (df['lease_commence_date'] + 99) - df['sale_year']\n",
    "\n",
    "# --- 2.2 Engineer Ordinal Features ---\n",
    "def parse_storey(storey_range):\n",
    "    parts = storey_range.split(' TO ')\n",
    "    return (int(parts[0]) + int(parts[1])) / 2\n",
    "df['storey_avg'] = df['storey_range'].apply(parse_storey)\n",
    "\n",
    "# --- 2.3 Define Target ---\n",
    "TARGET_COLUMN = 'resale_price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. BRUTE FORCE SCENARIO DEFINITION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 3.1 Define Feature Sets to Test ---\n",
    "feature_sets = {\n",
    "    \"Full_Set\": {\n",
    "        \"num\": ['floor_area_sqm', 'remaining_lease', 'sale_year', 'storey_avg'],\n",
    "        \"cat\": ['town', 'flat_type', 'flat_model']\n",
    "    },\n",
    "    \"Simple_Set\": {\n",
    "        \"num\": ['floor_area_sqm', 'remaining_lease'],\n",
    "        \"cat\": ['town']\n",
    "    },\n",
    "    \"No_Year_Set\": {\n",
    "        \"num\": ['floor_area_sqm', 'remaining_lease', 'storey_avg'],\n",
    "        \"cat\": ['town', 'flat_type', 'flat_model']\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- 3.2 Define Train/Test Splits to Test ---\n",
    "test_splits = [0.3, 0.2] # (70/30 and 80/20)\n",
    "\n",
    "# --- 3.3 Master Results List ---\n",
    "grand_results = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=888)\n",
    "\n",
    "# --- Helper function to evaluate models (UPDATED to show Train/Test RMSE) ---\n",
    "def evaluate_model(name, pipeline, params, X_train, y_train_log, X_test, y_test_log):\n",
    "    gs = GridSearchCV(pipeline, params, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    gs.fit(X_train, y_train_log)\n",
    "    best_model = gs.best_estimator_\n",
    "\n",
    "    # Predict on TRAIN set\n",
    "    y_pred_log_train = best_model.predict(X_train)\n",
    "    y_pred_orig_train = np.expm1(y_pred_log_train)\n",
    "    y_train_orig = np.expm1(y_train_log)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_orig_train))\n",
    "    r2_train = r2_score(y_train_orig, y_pred_orig_train)\n",
    "\n",
    "    # Predict on TEST set\n",
    "    y_pred_log_test = best_model.predict(X_test)\n",
    "    y_pred_orig_test = np.expm1(y_pred_log_test)\n",
    "    y_test_orig = np.expm1(y_test_log)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig_test))\n",
    "    r2_test = r2_score(y_test_orig, y_pred_orig_test)\n",
    "\n",
    "    print(f\"  {name} Complete. Train RMSE: ${rmse_train:,.2f} | Test RMSE: ${rmse_test:,.2f} | Test R2: {r2_test:.4f}\")\n",
    "    return {\n",
    "        'Train_RMSE': rmse_train, 'Test_RMSE': rmse_test,\n",
    "        'Train_R2': r2_train, 'Test_R2': r2_test,\n",
    "        'Best_Params': gs.best_params_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RUNNING SCENARIO: Full_Set (Split: 70/30) ---\n",
      "  OLS Complete. Train RMSE: $70,999.13 | Test RMSE: $71,224.58 | Test R2: 0.8451\n",
      "  Ridge Complete. Train RMSE: $70,999.13 | Test RMSE: $71,224.66 | Test R2: 0.8451\n",
      "  Lasso Complete. Train RMSE: $71,361.68 | Test RMSE: $71,574.23 | Test R2: 0.8436\n",
      "  Elastic Net Complete. Train RMSE: $73,347.54 | Test RMSE: $73,512.00 | Test R2: 0.8350\n",
      "  KNN Complete. Train RMSE: $31,043.48 | Test RMSE: $34,634.15 | Test R2: 0.9634\n",
      "  Bagging Complete. Train RMSE: $17,464.32 | Test RMSE: $28,883.62 | Test R2: 0.9745\n",
      "  Random Forest Complete. Train RMSE: $31,090.20 | Test RMSE: $34,638.33 | Test R2: 0.9634\n",
      "  Neural Network Complete. Train RMSE: $33,075.60 | Test RMSE: $33,331.65 | Test R2: 0.9661\n",
      "  Fitting Polynomial (Interaction) Model... (This may be slow)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.6 GiB for an array with shape (609000, 2774) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Fitting Polynomial (Interaction) Model... (This may be slow)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m poly_pipeline_full = Pipeline(steps=[\n\u001b[32m     90\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m'\u001b[39m, preprocessor_power), \u001b[38;5;66;03m# Use the 'power' preprocessor\u001b[39;00m\n\u001b[32m     91\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpoly\u001b[39m\u001b[33m'\u001b[39m, PolynomialFeatures(degree=\u001b[32m2\u001b[39m, include_bias=\u001b[38;5;28;01mFalse\u001b[39;00m, interaction_only=\u001b[38;5;28;01mFalse\u001b[39;00m)),\n\u001b[32m     92\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mregressor\u001b[39m\u001b[33m'\u001b[39m, LinearRegression())\n\u001b[32m     93\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mpoly_pipeline_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_log\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m y_pred_log_poly_train = poly_pipeline_full.predict(X_train)\n\u001b[32m     96\u001b[39m y_pred_log_poly_test = poly_pipeline_full.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    657\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    658\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    659\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    660\u001b[39m             all_params=params,\n\u001b[32m    661\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:682\u001b[39m, in \u001b[36mLinearRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    680\u001b[39m     \u001b[38;5;66;03m# cut-off ratio for small singular values\u001b[39;00m\n\u001b[32m    681\u001b[39m     cond = \u001b[38;5;28mmax\u001b[39m(X.shape) * np.finfo(X.dtype).eps\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     \u001b[38;5;28mself\u001b[39m.coef_, _, \u001b[38;5;28mself\u001b[39m.rank_, \u001b[38;5;28mself\u001b[39m.singular_ = \u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m     \u001b[38;5;28mself\u001b[39m.coef_ = \u001b[38;5;28mself\u001b[39m.coef_.T\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\_basic.py:1470\u001b[39m, in \u001b[36mlstsq\u001b[39m\u001b[34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m real_data:\n\u001b[32m   1469\u001b[39m     lwork, iwork = _compute_lwork(lapack_lwork, m, n, nrhs, cond)\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m     x, s, rank, info = \u001b[43mlapack_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43miwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# complex data\u001b[39;00m\n\u001b[32m   1473\u001b[39m     lwork, rwork, iwork = _compute_lwork(lapack_lwork, m, n,\n\u001b[32m   1474\u001b[39m                                          nrhs, cond)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 12.6 GiB for an array with shape (609000, 2774) and data type float64"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. START BRUTE FORCE LOOP\n",
    "# ==============================================================================\n",
    "overall_start_time = time.time()\n",
    "\n",
    "for set_name, features in feature_sets.items():\n",
    "    for test_size in test_splits:\n",
    "\n",
    "        scenario_name = f\"{set_name} (Split: {int((1-test_size)*100)}/{int(test_size*100)})\"\n",
    "        print(f\"\\n--- RUNNING SCENARIO: {scenario_name} ---\")\n",
    "\n",
    "        # --- 4.1 Define Features for this Scenario ---\n",
    "        NUMERICAL_FEATURES = features['num']\n",
    "        CATEGORICAL_FEATURES = features['cat']\n",
    "        ALL_FEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n",
    "\n",
    "        # --- 4.2 Clean and Split Data for this Scenario ---\n",
    "        df_clean = df[ALL_FEATURES + [TARGET_COLUMN]].dropna()\n",
    "        X = df_clean[ALL_FEATURES]\n",
    "        y = df_clean[TARGET_COLUMN]\n",
    "        y_log = np.log1p(y)\n",
    "\n",
    "        X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
    "            X, y_log, test_size=test_size, random_state=888\n",
    "        )\n",
    "        y_train_orig = np.expm1(y_train_log)\n",
    "        y_test_orig = np.expm1(y_test_log)\n",
    "\n",
    "        # --- 4.3 Define Preprocessing Pipelines for this Scenario ---\n",
    "        power_transformer_pipeline = Pipeline(steps=[\n",
    "            ('power', PowerTransformer(method='yeo-johnson')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        preprocessor_power = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', power_transformer_pipeline, NUMERICAL_FEATURES),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CATEGORICAL_FEATURES)\n",
    "            ], remainder='passthrough'\n",
    "        )\n",
    "        preprocessor_tree = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', 'passthrough', NUMERICAL_FEATURES),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CATEGORICAL_FEATURES)\n",
    "            ], remainder='passthrough'\n",
    "        )\n",
    "        # Pre-process data for non-pipeline models (Stepwise, etc.)\n",
    "        X_train_processed_power = preprocessor_power.fit_transform(X_train)\n",
    "        X_test_processed_power = preprocessor_power.transform(X_test)\n",
    "\n",
    "        # --- 4.4 Run Model Gauntlet for this Scenario ---\n",
    "        model_results = {}\n",
    "\n",
    "        # --- A. Linear Regression (OLS) ---\n",
    "        ols_pipeline = Pipeline(steps=[('preprocessor', preprocessor_power), ('regressor', LinearRegression())])\n",
    "        model_results['OLS'] = evaluate_model('OLS', ols_pipeline, {}, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "        # --- B. Ridge Regression ---\n",
    "        ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor_power), ('regressor', Ridge(random_state=888))])\n",
    "        model_results['Ridge'] = evaluate_model('Ridge', ridge_pipeline, {'regressor__alpha': np.logspace(-2, 3, 6)}, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "        # --- C. Lasso Regression ---\n",
    "        lasso_pipeline = Pipeline(steps=[('preprocessor', preprocessor_power), ('regressor', Lasso(random_state=888, max_iter=2000))])\n",
    "        model_results['Lasso'] = evaluate_model('Lasso', lasso_pipeline, {'regressor__alpha': np.logspace(-4, -1, 5)}, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "        # --- D. Elastic Net Regression ---\n",
    "        elastic_pipeline = Pipeline(steps=[('preprocessor', preprocessor_power), ('regressor', ElasticNet(random_state=888, max_iter=2000))])\n",
    "        model_results['Elastic Net'] = evaluate_model('Elastic Net', elastic_pipeline, {'regressor__alpha': [0.001], 'regressor__l1_ratio': [0.5]}, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "        # --- E. K-Nearest Neighbors (KNN) Regressor ---\n",
    "        knn_pipeline = Pipeline(steps=[('preprocessor', preprocessor_power), ('regressor', KNeighborsRegressor())])\n",
    "        model_results['KNN'] = evaluate_model('KNN', knn_pipeline, {'regressor__n_neighbors': [10]}, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "        # --- F. Bagging Regressor ---\n",
    "        bagging_pipeline = Pipeline(steps=[('preprocessor', preprocessor_tree), ('regressor', BaggingRegressor(random_state=888))])\n",
    "        model_results['Bagging'] = evaluate_model('Bagging', bagging_pipeline, {'regressor__n_estimators': [50]}, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "        # --- G. Random Forest Regressor ---\n",
    "        rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor_tree), ('regressor', RandomForestRegressor(random_state=888))])\n",
    "        model_results['Random Forest'] = evaluate_model('Random Forest', rf_pipeline, {'regressor__n_estimators': [100], 'regressor__max_depth': [20]}, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "        # --- H. Neural Network (MLP Regressor) ---\n",
    "        mlp_pipeline = Pipeline(steps=[('preprocessor', preprocessor_power), ('regressor', MLPRegressor(random_state=888, max_iter=500, early_stopping=True))])\n",
    "        model_results['Neural Network'] = evaluate_model('Neural Network', mlp_pipeline, {'regressor__hidden_layer_sizes': [(100, 50)], 'regressor__alpha': [0.001]}, X_train, y_train_log, X_test, y_test_log)\n",
    "\n",
    "        # --- WARNING: The following models are EXTREMELY slow ---\n",
    "\n",
    "        # --- I. Polynomial / Interaction Regression ---\n",
    "        print(f\"  Fitting Polynomial (Interaction) Model... (This may be slow)\")\n",
    "        poly_pipeline_full = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor_power), # Use the 'power' preprocessor\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "        poly_pipeline_full.fit(X_train, y_train_log)\n",
    "        y_pred_log_poly_train = poly_pipeline_full.predict(X_train)\n",
    "        y_pred_log_poly_test = poly_pipeline_full.predict(X_test)\n",
    "        rmse_poly_train = np.sqrt(mean_squared_error(y_train_orig, np.expm1(y_pred_log_poly_train)))\n",
    "        rmse_poly_test = np.sqrt(mean_squared_error(y_test_orig, np.expm1(y_pred_log_poly_test)))\n",
    "        r2_poly_train = r2_score(y_train_orig, np.expm1(y_pred_log_poly_train))\n",
    "        r2_poly_test = r2_score(y_test_orig, np.expm1(y_pred_log_poly_test))\n",
    "        model_results['Polynomial/Interaction'] = {'Train_RMSE': rmse_poly_train, 'Test_RMSE': rmse_poly_test, 'Train_R2': r2_poly_train, 'Test_R2': r2_poly_test, 'Best_Params': 'degree=2'}\n",
    "        print(f\"  Polynomial/Interaction Complete. Train RMSE: ${rmse_poly_train:,.2f} | Test RMSE: ${rmse_poly_test:,.2f} | Test R2: {r2_poly_test:.4f}\")\n",
    "\n",
    "        # --- J. Forward Stepwise Regression ---\n",
    "        print(f\"  Fitting Forward Stepwise Model... (This is very slow)\")\n",
    "        lr = LinearRegression()\n",
    "        sfs_forward = SequentialFeatureSelector(lr, n_features_to_select='auto', direction='forward', tol=None, cv=3, n_jobs=-1) # cv=3 for speed\n",
    "        sfs_forward.fit(X_train_processed_power, y_train_log)\n",
    "        X_train_sfs_f = sfs_forward.transform(X_train_processed_power)\n",
    "        X_test_sfs_f = sfs_forward.transform(X_test_processed_power)\n",
    "\n",
    "        lr_sfs_f = LinearRegression().fit(X_train_sfs_f, y_train_log)\n",
    "        y_pred_log_sfs_f_train = lr_sfs_f.predict(X_train_sfs_f)\n",
    "        y_pred_log_sfs_f_test = lr_sfs_f.predict(X_test_sfs_f)\n",
    "        rmse_sfs_f_train = np.sqrt(mean_squared_error(y_train_orig, np.expm1(y_pred_log_sfs_f_train)))\n",
    "        rmse_sfs_f_test = np.sqrt(mean_squared_error(y_test_orig, np.expm1(y_pred_log_sfs_f_test)))\n",
    "        r2_sfs_f_train = r2_score(y_train_orig, np.expm1(y_pred_log_sfs_f_train))\n",
    "        r2_sfs_f_test = r2_score(y_test_orig, np.expm1(y_pred_log_sfs_f_test))\n",
    "        model_results['Forward Stepwise'] = {'Train_RMSE': rmse_sfs_f_train, 'Test_RMSE': rmse_sfs_f_test, 'Train_R2': r2_sfs_f_train, 'Test_R2': r2_sfs_f_test, 'Best_Params': f'{X_train_sfs_f.shape[1]} features'}\n",
    "        print(f\"  Forward Stepwise Complete. Train RMSE: ${rmse_sfs_f_train:,.2f} | Test RMSE: ${rmse_sfs_f_test:,.2f} | Test R2: {r2_sfs_f_test:.4f}\")\n",
    "\n",
    "        # --- K. Backward Stepwise Regression ---\n",
    "        print(f\"  Fitting Backward Stepwise Model... (This is EXTREMELY slow, will cap at 40 features)\")\n",
    "        lr_back = LinearRegression()\n",
    "        \n",
    "        # Reduce memory usage by:\n",
    "        # 1. Limiting the number of features even more\n",
    "        n_features_backward = min(20, X_train_processed_power.shape[1] - 1)  # Reduced from 50 to 20\n",
    "        \n",
    "        # 2. Use a smaller subset of data for feature selection if dataset is large\n",
    "        sample_size = min(10000, X_train_processed_power.shape[0])\n",
    "        if X_train_processed_power.shape[0] > sample_size:\n",
    "            # Create a random sample for feature selection\n",
    "            random_indices = np.random.choice(X_train_processed_power.shape[0], sample_size, replace=False)\n",
    "            X_train_sample = X_train_processed_power[random_indices]\n",
    "            y_train_sample = y_train_log[random_indices]\n",
    "        else:\n",
    "            X_train_sample = X_train_processed_power\n",
    "            y_train_sample = y_train_log\n",
    "        \n",
    "        # 3. Reduce number of cross-validation folds\n",
    "        sfs_backward = SequentialFeatureSelector(lr_back, n_features_to_select=n_features_backward, \n",
    "                                               direction='backward', cv=2, n_jobs=1)  # Reduced CV and using single job\n",
    "        \n",
    "        # Fit on the sample\n",
    "        sfs_backward.fit(X_train_sample, y_train_sample)\n",
    "        X_train_sfs_b = sfs_backward.transform(X_train_processed_power)\n",
    "        X_test_sfs_b = sfs_backward.transform(X_test_processed_power)\n",
    "\n",
    "        lr_sfs_b = LinearRegression().fit(X_train_sfs_b, y_train_log)\n",
    "        y_pred_log_sfs_b_train = lr_sfs_b.predict(X_train_sfs_b)\n",
    "        y_pred_log_sfs_b_test = lr_sfs_b.predict(X_test_sfs_b)\n",
    "        rmse_sfs_b_train = np.sqrt(mean_squared_error(y_train_orig, np.expm1(y_pred_log_sfs_b_train)))\n",
    "        rmse_sfs_b_test = np.sqrt(mean_squared_error(y_test_orig, np.expm1(y_pred_log_sfs_b_test)))\n",
    "        r2_sfs_b_train = r2_score(y_train_orig, np.expm1(y_pred_log_sfs_b_train))\n",
    "        r2_sfs_b_test = r2_score(y_test_orig, np.expm1(y_pred_log_sfs_b_test))\n",
    "        model_results['Backward Stepwise'] = {'Train_RMSE': rmse_sfs_b_train, 'Test_RMSE': rmse_sfs_b_test, 'Train_R2': r2_sfs_b_train, 'Test_R2': r2_sfs_b_test, 'Best_Params': f'{X_train_sfs_b.shape[1]} features'}\n",
    "        print(f\"  Backward Stepwise Complete. Train RMSE: ${rmse_sfs_b_train:,.2f} | Test RMSE: ${rmse_sfs_b_test:,.2f} | Test R2: {r2_sfs_b_test:.4f}\")\n",
    "\n",
    "        # --- 4.5 Store Scenario Results ---\n",
    "        scenario_df = pd.DataFrame(model_results).T\n",
    "        scenario_df['scenario'] = scenario_name\n",
    "        scenario_df['model'] = scenario_df.index\n",
    "        grand_results.append(scenario_df)\n",
    "\n",
    "        print(f\"--- SCENARIO COMPLETE: {scenario_name} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. FINAL BRUTE FORCE RESULTS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BRUTE FORCE ANALYSIS COMPLETE\")\n",
    "print(f\"Total time taken: {(time.time() - overall_start_time)/60:.2f} minutes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Concatenate all results into one big DataFrame\n",
    "final_results_df = pd.concat(grand_results)\n",
    "\n",
    "# --- 5.1 Print the Grand Champion Table ---\n",
    "print(\"\\n--- Grand Results Table (All Scenarios) ---\")\n",
    "final_table = final_results_df[\n",
    "    ['scenario', 'model', 'Train_RMSE', 'Test_RMSE', 'Test_R2']\n",
    "].sort_values(by='Test_RMSE')\n",
    "print(final_table.to_string()) # .to_string() prints the full table\n",
    "\n",
    "# --- 5.2 Find and Recommend the Absolute Best Model ---\n",
    "best_idx = final_results_df['Test_RMSE'].idxmin()\n",
    "best_overall_run = final_results_df.loc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RECOMMENDATION (GRAND CHAMPION)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"The best overall model/scenario combination is:\")\n",
    "print(f\"  Scenario: **{best_overall_run['scenario']}**\")\n",
    "print(f\"  Model: **{best_overall_run['model']}**\")\n",
    "\n",
    "# Convert values to Python scalars before formatting\n",
    "print(f\"\\n  Lowest Test RMSE: **${best_overall_run['Test_RMSE'].values[0]:,.2f}**\")\n",
    "print(f\"  Test R-squared: **{best_overall_run['Test_R2'].values[0]:.4f}**\")\n",
    "print(f\"  Train RMSE: ${best_overall_run['Train_RMSE'].values[0]:,.2f} (Check for overfitting)\")\n",
    "\n",
    "\n",
    "# Handle the Best_Params which might be a complex object\n",
    "print(f\"  Best parameters: {best_overall_run.get('Best_Params', 'N/A')}\")\n",
    "\n",
    "print(\"\\nJustification: After running a comprehensive tournament of 11 models across 6 different\")\n",
    "print(f\"scenarios (3 feature sets x 2 train/test splits), the combination of\")\n",
    "print(f\"**{best_overall_run['model']}** on the **'{best_overall_run['scenario']}'**\")\n",
    "print(f\"provided the lowest test error, indicating the best combination\")\n",
    "print(f\"of features, split, and algorithm for generalization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. FINAL DEEP DIVE on the Winning Scenario\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"STARTING DEEP DIVE ON WINNING SCENARIO: {best_overall_run['scenario']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 6.1 Re-create the winning scenario's data ---\n",
    "# First, access the specific string value from the 'scenario' Series/column\n",
    "# We use .iloc[0] to get the value from the first row\n",
    "scenario_string = best_overall_run['scenario'].iloc[0]\n",
    "\n",
    "# Now you can safely split the string\n",
    "winning_set_name = scenario_string.split(\" (\")[0]\n",
    "winning_split = float(scenario_string.split(\"/\")[1][:2]) / 100.0\n",
    "winning_features = feature_sets[winning_set_name]\n",
    "\n",
    "# You can print these to verify\n",
    "print(f\"Winning Set Name: {winning_set_name}\")\n",
    "print(f\"Winning Split: {winning_split}\")\n",
    "\n",
    "NUMERICAL_FEATURES = winning_features['num']\n",
    "CATEGORICAL_FEATURES = winning_features['cat']\n",
    "ALL_FEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n",
    "df_clean = df[ALL_FEATURES + [TARGET_COLUMN]].dropna()\n",
    "X = df_clean[ALL_FEATURES]\n",
    "y = df_clean[TARGET_COLUMN]\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
    "    X, y_log, test_size=winning_split, random_state=888\n",
    ")\n",
    "\n",
    "# --- 6.2 Re-create the winning preprocessor ---\n",
    "preprocessor_power_final = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[('power', PowerTransformer(method='yeo-johnson')), ('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CATEGORICAL_FEATURES)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- 6.3 OLS Summary (Equivalent to R's summary(lm_model)) ---\n",
    "print(\"\\n--- OLS Model Summary (from statsmodels) ---\")\n",
    "X_train_processed_power = preprocessor_power_final.fit_transform(X_train)\n",
    "X_test_processed_power = preprocessor_power_final.transform(X_test)\n",
    "feature_names_out = preprocessor_power_final.get_feature_names_out()\n",
    "X_train_sm = sm.add_constant(X_train_processed_power, prepend=False)\n",
    "\n",
    "sm_model = sm.OLS(y_train_log, X_train_sm).fit()\n",
    "print(sm_model.summary(xname=['const'] + list(feature_names_out)))\n",
    "print(\"\\nInterpretation: Review p-values (P>|t|) to see which features are statistically significant.\")\n",
    "\n",
    "# --- Residual Plots ---\n",
    "y_pred_log_train = sm_model.predict(X_train_sm)\n",
    "ols_residuals_log = y_train_log - y_pred_log_train\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(x=y_pred_log_train, y=ols_residuals_log, alpha=0.5)\n",
    "plt.hlines(y=0, xmin=min(y_pred_log_train), xmax=max(y_pred_log_train), color='red', linestyle='--')\n",
    "plt.title('Residuals vs Fitted Values (Log Scale)')\n",
    "plt.show()\n",
    "\n",
    "sm.qqplot(ols_residuals_log, line='45', fit=True)\n",
    "plt.title('Q-Q Plot of Residuals (Log Scale)')\n",
    "plt.show()\n",
    "print(\"Interpretation: Check plots for randomness and normality.\")\n",
    "\n",
    "# --- 6.5 Multicollinearity (VIF) ---\n",
    "print(\"\\n--- Calculating VIF scores ---\")\n",
    "# Make sure we're using a list of column names here\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed_power, columns=feature_names_out)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_train_processed_df.columns\n",
    "vif_scores = []\n",
    "for i in range(X_train_processed_df.shape[1]):\n",
    "    vif_scores.append(variance_inflation_factor(X_train_processed_df.values, i))\n",
    "vif_data[\"VIF\"] = vif_scores\n",
    "print(\"\\nVIF Scores (Top 10):\")\n",
    "print(vif_data.sort_values(by='VIF', ascending=False))\n",
    "print(\"Interpretation: VIF scores > 5 or 10 suggest multicollinearity.\")\n",
    "\n",
    "# --- 6.6 Bootstrap for Coefficient Stability ---\n",
    "print(\"\\n--- Bootstrapping OLS Coefficients (100 iterations) ---\")\n",
    "n_iterations = 100\n",
    "n_size = int(len(X_train_processed_power) * 0.50)\n",
    "boot_coefs = []\n",
    "for i in range(n_iterations):\n",
    "    X_sample, y_sample = resample(X_train_processed_power, y_train_log, n_samples=n_size, random_state=i)\n",
    "    boot_model = LinearRegression().fit(X_sample, y_sample)\n",
    "    boot_coefs.append(boot_model.coef_)\n",
    "\n",
    "boot_coefs_df = pd.DataFrame(boot_coefs, columns=feature_names_out)\n",
    "print(\"Bootstrap Coefficient Summary (Mean and Std Dev):\")\n",
    "print(boot_coefs_df.describe().loc[['mean', 'std']].T)\n",
    "print(\"Interpretation: Low standard deviations suggest stable coefficients.\")\n",
    "\n",
    "print(\"\\n--- FULL ANALYSIS COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
